{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c80dbade",
   "metadata": {},
   "source": [
    "# Chapter5: Streaming Deployment\n",
    "\n",
    ">This notebook is based on the open-source project [wow-rag](https://github.com/datawhalechina/wow-rag) by Datawhale China.  \n",
    ">I’ve adapted and annotated parts of it for personal learning and experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221aa61d",
   "metadata": {},
   "source": [
    "## 1. Introduction : What does \"streaming deployment\" mean?\n",
    "\n",
    "Streaming deployment refers to  where responses from an LLM are generated and displayed incrementally (token by token or sentence by sentence) instead of waiting for the full output at once. \n",
    "\n",
    "This creates a more interactive user experience, especially for long or complex responses.\n",
    "\n",
    "**Why do we need it?**\n",
    "\n",
    "- Faster Feedback: Users see partial results immediately instead of waiting.\n",
    "\n",
    "- Interactive Feel: It simulates a real-time conversation, making chatbots feel more responsive.\n",
    "\n",
    "- Scalable UX: Especially useful in web interfaces where latency matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a310f",
   "metadata": {},
   "source": [
    "## 2. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce55cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "\n",
    "#base_url = \"hhttps://api.openai.com/v1\"  # We use openai's model here\n",
    "chat_model = \"gpt-4.1-nano-2025-04-14\"   # We will be using cheaper model as im broke AF\n",
    "emb_model = \"text-embedding-3-small\"\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key = api_key,\n",
    "    #base_url = base_url\n",
    ")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(\n",
    "    api_key = api_key,\n",
    "    model = chat_model,\n",
    ")\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "embedding = OpenAIEmbedding(\n",
    "    api_key = api_key,\n",
    "    model = emb_model,\n",
    ")\n",
    "emb = embedding.get_text_embedding(\"Hellooo\")\n",
    "\n",
    "\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader,Document\n",
    "documents = SimpleDirectoryReader(input_files=['./docs/example.txt']).load_data()\n",
    "\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex.from_documents(documents,embed_model=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda51c5d",
   "metadata": {},
   "source": [
    "## 3.  Streaming Responses with `response_gen`\n",
    "\n",
    "\n",
    "We're using `query_engine = index.as_query_engine(streaming=True)` to simulate a **real-time generation experience** — ideal for chatbot-like interfaces or interactive writing assistants.\n",
    "\n",
    "###  Alternative Methods:\n",
    "- **No Streaming**: Use `streaming=False` and wait for the entire result, then print.\n",
    "- **FastAPI Backend**: Use a backend server to stream via `StreamingResponse` (e.g., for web apps).\n",
    "- **LangChain**: An alternative to LlamaIndex, also supports streaming with various models.\n",
    "- **Direct OpenAI API**: If using OpenAI's models, you can stream tokens via their `stream=True` parameter in `openai.ChatCompletion.create()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2c4dca",
   "metadata": {},
   "source": [
    "### 3.1 Construct query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcf6f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    streaming=True, \n",
    "    similarity_top_k=3,\n",
    "    llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3fa80",
   "metadata": {},
   "source": [
    "### 3.2 Construct Stream reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b5e3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The employment prospects for AI majors are promising, as the technology continues to expand across various industries.\n",
      "\n",
      "AI specialists are in high demand for roles in healthcare, gaming, manufacturing, and content generation, where they develop and refine intelligent systems.\n",
      "\n",
      "Responsible development and ethical considerations are increasingly prioritized, creating opportunities for experts in AI ethics and safety.\n",
      "\n",
      "However, professionals must stay adaptable, as the field evolves rapidly with new applications and challenges.\n",
      "\n",
      "Continuous learning and understanding of multimodal models, collaboration policies, and safety measures will be essential for AI majors seeking to thrive in this dynamic job market.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response_stream = query_engine.query(\"Please write a 100-word article on the employment prospects of AI majors\") \n",
    "buffer = \"\" # Buffer for auto line wrap\n",
    "for text in response_stream.response_gen: # Stream the output as it's generated\n",
    "    buffer += text\n",
    "    while \".\" in buffer:\n",
    "        sentence, buffer = buffer.split(\".\", 1)\n",
    "        print(sentence.strip() + \".\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d22ef",
   "metadata": {},
   "source": [
    "## 4. Streaming with `FastAPI`\n",
    "\n",
    "### FastAPI: A Modern Web Framework for AI Backends\n",
    "\n",
    "\n",
    "FastAPI is a high-performance web framework for building APIs with Python 3.7+, based on standard Python type hints.\n",
    "\n",
    "It’s widely used to deploy machine learning models, including LLM-based services, by wrapping them in a RESTful API or WebSocket backend.\n",
    "\n",
    "---\n",
    "\n",
    "###  Why Use FastAPI?\n",
    "\n",
    "Use FastAPI when you want to:\n",
    "\n",
    "-  Build a full backend API for your LLM or vector search app  \n",
    "-  Integrate with frontends or other services (e.g., JavaScript, React, mobile apps)  \n",
    "-  Serve models or embeddings remotely, rather than keeping them embedded in notebooks  \n",
    "\n",
    "---\n",
    "\n",
    "###  When to Use FastAPI?\n",
    "\n",
    "- Building a **production-grade** AI application  \n",
    "- Integrating with a **frontend UI** (Streamlit, React, Vue, etc.)  \n",
    "- Exposing your model to **external users or services**  \n",
    "- **Scaling** to multiple users or concurrent requests  \n",
    "\n",
    "---\n",
    "\n",
    "###  Uvicorn :  ASGI web server for Python\n",
    "\n",
    "**Uvicorn** is a lightning-fast **ASGI web server** for Python — it's what actually runs our **FastAPI** app when deployed.\n",
    "\n",
    "###  Key Points:\n",
    "- Uvicorn stands for **\"Universal Interface for ASGI applications running on asyncio\"**.\n",
    "- It runs your FastAPI app by listening to HTTP requests and routing them to your Python code.\n",
    "- Unlike older WSGI servers (like Flask uses), **ASGI** supports **asynchronous programming**, which is perfect for handling many concurrent requests — like a chatbot or LLM app.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89744cde",
   "metadata": {},
   "source": [
    "### 4.1 Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9c0767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install fastapi\n",
    "# %pip install uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc3738d",
   "metadata": {},
   "source": [
    "### 4.2 FastAPI Streaming Server Example \n",
    "\n",
    "This example shows how to set up a basic FastAPI server that streams responses from a language model  or query engine in real time using Server-Sent Events (SSE). It's useful for building chatbot backends or interactive web apps with live outputs.\n",
    "\n",
    "The code includes:\n",
    "- FastAPI app creation and CORS setup\n",
    "- A background thread to run the server from a notebook\n",
    "- A streaming endpoint (`/stream_chat`) to serve token-by-token responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee484db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lunched：http://localhost:5000/stream_chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [15120]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# == Import required libraries ==\n",
    "\n",
    "import uvicorn  # ASGI server used to run FastAPI apps\n",
    "from fastapi import FastAPI  # Main web framework\n",
    "from fastapi.middleware.cors import CORSMiddleware  # To allow cross-origin requests (e.g., frontend calls)\n",
    "from fastapi.responses import StreamingResponse  # Allows returning streaming responses (like token-by-token LLM output)\n",
    "import threading  # For running the server in a background thread (useful inside notebooks)\n",
    "\n",
    "# == Initialize FastAPI app ==\n",
    "app = FastAPI()\n",
    "\n",
    "# == Allow all cross-origin requests (good for prototyping, but lock it down for production) ==\n",
    "app.add_middleware(CORSMiddleware, allow_origins=[\"*\"])\n",
    "\n",
    "# == Server thread placeholder ==\n",
    "_server_thread = None\n",
    "\n",
    "# == Function to run the FastAPI server (in the background) ==\n",
    "def run_server():\n",
    "    config = uvicorn.Config(app, host='0.0.0.0', port=5000)  # Run on all interfaces (0.0.0.0), port 5000\n",
    "    server = uvicorn.Server(config)\n",
    "    server.run()\n",
    "\n",
    "# == Start the server in a separate thread (non-blocking, good for Jupyter/Colab) ==\n",
    "def start_server():\n",
    "    global _server_thread\n",
    "    if not _server_thread or not _server_thread.is_alive():\n",
    "        _server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "        _server_thread.start()\n",
    "        print(\"Lunched：http://localhost:5000/stream_chat\")\n",
    "\n",
    "# == Define a route for streaming chat ==\n",
    "@app.get('/stream_chat')\n",
    "async def stream_chat(param: str = \"Hello\"):\n",
    "    \"\"\"\n",
    "    A GET endpoint that accepts a string parameter param and returns the streaming response of the language model.\n",
    "    \"\"\"\n",
    "    async def generate():\n",
    "        response_stream = query_engine.query(param)  \n",
    "        for text in response_stream.response_gen:    \n",
    "            yield text  # Use yield to generate each output segment to form a streaming response\n",
    "    return StreamingResponse(generate(), media_type='text/event-stream')  # Returns SSE response, suitable for real-time display on web pages\n",
    "\n",
    "# == Start the server (call it directly in Notebook) ==\n",
    "start_server()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cab150",
   "metadata": {},
   "source": [
    "### 4.3 Calling the FastAPI Streaming Endpoint (Client Side)\n",
    "\n",
    "This example shows how to **send a request** to the `/stream_chat` endpoint we defined earlier using Python’s `requests` library.\n",
    "\n",
    "The response is streamed back in **real time**, perfect for handling outputs from a chatbot or LLM that emits results token-by-token or line-by-line.\n",
    "\n",
    "The code:\n",
    "- Makes a `GET` request to the local FastAPI server\n",
    "- Streams the response chunks gradually\n",
    "- Prints them live to your terminal or notebook output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d338ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:56836 - \"GET /stream_chat?param=What+are+the+employment+prospects+for+AI+majors%3F HTTP/1.1\" 200 OK\n",
      "The provided information does not include specific details about employment prospects for AI majors."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:57028 - \"GET /stream_chat?param=%E4%BD%A0%E6%98%AF%E8%B0%81%EF%BC%9F HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:57028 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"
     ]
    }
   ],
   "source": [
    "# == Import requests library to send HTTP requests ==\n",
    "import requests\n",
    "\n",
    "# == Define a function to send a streaming request to FastAPI ==\n",
    "def test_stream_chat(question=\"你好\"):\n",
    "    url = \"http://localhost:5000/stream_chat\"  # URL of the local FastAPI endpoint\n",
    "    params = {\"param\": question}  # Query string parameter expected by the server\n",
    " \n",
    "    # Make a streaming GET request\n",
    "    with requests.get(url, params=params, stream=True) as response:\n",
    "        # Iterate over the streamed content line-by-line\n",
    "        for chunk in response.iter_content(decode_unicode=True):\n",
    "            if chunk:\n",
    "                print(chunk, end=\"\", flush=True)  # Print without newline & flush buffer to show live text\n",
    "\n",
    "# == Call the test function with an example query ==\n",
    "test_stream_chat(\"What are the employment prospects for AI majors?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aad44e",
   "metadata": {},
   "source": [
    "## 5. From Backend to Frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d0532d",
   "metadata": {},
   "source": [
    "We can create a python file (here `main.py`) with following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17980fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import uvicorn\n",
    "# from fastapi import FastAPI\n",
    "# from fastapi.middleware.cors import CORSMiddleware\n",
    "# from fastapi.responses import StreamingResponse\n",
    "# app = FastAPI()\n",
    "# app.add_middleware(CORSMiddleware,allow_origins=[\"*\"])\n",
    "# @app.get('/stream_chat')\n",
    "# async def stream_chat(param:str = \"你好\"):\n",
    "#     def generate():  \n",
    "#         # 我们假设query_engine已经构建完成\n",
    "#         response_stream = query_engine.query(param) \n",
    "#         for text in response_stream.response_gen:\n",
    "#             yield text\n",
    "#     return StreamingResponse(generate(), media_type='text/event-stream')  \n",
    "# if __name__ == '__main__':\n",
    "#     uvicorn.run(app, host='0.0.0.0', port=5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d14bac",
   "metadata": {},
   "source": [
    "The only difference between running this code in a Jupyter cell and running it in a `.py` file lies in the part after if `__name__ == '__main__'`:. This is because Jupyter is an interactive environment where code is executed cell by cell, rather than as a standalone program.\n",
    "\n",
    "As a result, when we run a program in Jupyter, it runs in a new process instead of the main one. That's why in Jupyter, we need to use uvicorn.Server inside the if `__name__ == '__main__'`: block to manually start the server. However, this step is unnecessary when running the code from a regular Python file.\n",
    "\n",
    "We can even open a browser and directly enter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b535782",
   "metadata": {},
   "source": [
    "```http://127.0.0.1:5000/stream_chat?param=Who are you?```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70454e",
   "metadata": {},
   "source": [
    "the browser will then display the streaming output in real time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

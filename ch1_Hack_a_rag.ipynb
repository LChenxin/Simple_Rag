{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02ecc9c",
   "metadata": {},
   "source": [
    "# Lesson 1 Hack-A-Rag\n",
    "\n",
    ">This notebook is based on the open-source project [wow-rag](https://github.com/datawhalechina/wow-rag) by Datawhale China.  \n",
    ">I‚Äôve adapted and annotated parts of it for personal learning and experimentation.\n",
    "\n",
    "## 1. Introduction of Retrieval-Augmented Generation (RAG) \n",
    "\n",
    "**RAG** (Retrieval-Augmented Generation) is a technique that enhances a language model by combining it with a retriever component. It allows the model to fetch relevant documents or knowledge **at inference time**, making it more accurate, up-to-date, and less prone to hallucination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc30fb04",
   "metadata": {},
   "source": [
    "###  How RAG Works\n",
    "\n",
    "1. **Retriever**: Finds relevant documents or chunks based on the input question.\n",
    "2. **Generator**: A language model (e.g., GPT) that generates a response using both the query and the retrieved documents.\n",
    "\n",
    "User Query ‚Üí [Retriever] ‚Üí Top-k Text Chunks ‚Üí [Generator] ‚Üí Final Answer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13c937",
   "metadata": {},
   "source": [
    "### Use Cases of Rag\n",
    "| Domain           | Example                                              |\n",
    "| ---------------- | ---------------------------------------------------- |\n",
    "| Customer Support | Answering product questions using internal docs      |\n",
    "| Legal / Finance  | Q\\&A over contracts or compliance manuals            |\n",
    "| Healthcare       | Accessing medical guidelines or patient records      |\n",
    "| Education        | Personalized tutoring using a curated knowledge base |\n",
    "| Research         | Summarizing and answering from scientific papers     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddc8614",
   "metadata": {},
   "source": [
    "### RAG vs SFT \n",
    "\n",
    "| Aspect              | RAG                                               | SFT (Fine-tuning)                                 |\n",
    "| ------------------- | ------------------------------------------------- | ------------------------------------------------- |\n",
    "| üîß Setup            | Plug external knowledge into a frozen model       | Retrain the model with task-specific labeled data |\n",
    "| üìö Knowledge Source | External documents (updated any time)             | Internal weights (static knowledge)               |\n",
    "| üîÑ Updatable?       | ‚úÖ Yes ‚Äì update documents, no retraining needed    | ‚ùå No ‚Äì requires retraining to update knowledge    |\n",
    "| üß† Generalization   | Strong with relevant context                      | Strong if trained well, but fixed to domain       |\n",
    "| üß™ Sample Use       | Open-book QA, long documents, real-time knowledge | Closed-domain tasks, chatbots, classification     |\n",
    "| üí∞ Cost             | Inference-time retrieval + LLM API                | Expensive training, but cheap inference           |\n",
    "| üìà Scalability      | Easily scales to new domains via document updates | Requires labeled data for every new task          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed2440",
   "metadata": {},
   "source": [
    "## 2. Build-a-rag !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3aac58",
   "metadata": {},
   "source": [
    "### 2.1 Install required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f9555e",
   "metadata": {},
   "source": [
    "####  What is `faiss`?\n",
    "\n",
    "- **What it is:** Facebook AI Similarity Search\n",
    "- **Use:** Efficient similarity search for high-dimensional vectors (e.g., embeddings)\n",
    "- **Why we need it:** We use FAISS to index and retrieve the most relevant chunks based on vector similarity.\n",
    "\n",
    "| Feature          | `faiss-cpu`                          | `faiss-gpu`                            |\n",
    "| ---------------- | ------------------------------------ | -------------------------------------- |\n",
    "| Platform         | CPU                                  | CUDA-enabled GPU                       |\n",
    "| Speed            | Slower (especially for large data)   | Much faster on large datasets          |\n",
    "| Memory usage     | Uses system RAM                      | Uses GPU VRAM                          |\n",
    "| Dataset size     | Good for small/medium (<1M vectors)  | Suitable for large-scale search (10M+) |\n",
    "| Installation     | Easy via pip                         | Requires NVIDIA GPU + CUDA setup       |\n",
    "| Typical use case | Research, notebooks, lightweight RAG | Production, web-scale retrieval        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8874c596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cernet.edu.cn/pypi/web/simpleNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\liuch\\miniconda3\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\liuch\\miniconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\liuch\\miniconda3\\lib\\site-packages (1.11.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from faiss-cpu) (1.25.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from faiss-cpu) (23.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Looking in indexes: https://mirrors.cernet.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: openai in c:\\users\\liuch\\miniconda3\\lib\\site-packages (1.95.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://mirrors.cernet.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\liuch\\miniconda3\\lib\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu scikit-learn scipy \n",
    "%pip install openai \n",
    "%pip install python-dotenv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c18986",
   "metadata": {},
   "source": [
    "### 2.2 Environment and >.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e271912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "\n",
    "base_url = \"hhttps://api.openai.com/v1\"  # We use openai's model here\n",
    "chat_model = \"gpt-4.1-nano-2025-04-14\"   # We will be using cheaper model as im broke AF\n",
    "emb_model = \"text-embedding-3-small\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d5892",
   "metadata": {},
   "source": [
    "### 2.3 Construct Client\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,       # Required: Your OpenAI API key\n",
    "    base_url=None,         # Optional: Override endpoint (e.g. Azure or self-hosted)\n",
    "    organization=None,     # Optional: Your OpenAI organization ID\n",
    "    timeout=None           # Optional: Set custom timeout (in seconds)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d75a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key = api_key,\n",
    "    #base_url = base_url\n",
    ")\n",
    "\n",
    "is_valid = True if client.models.list() else False # True if the setup is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266445fc",
   "metadata": {},
   "source": [
    "### 2.4 Construct the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f88f257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_text = \"\"\"\n",
    "\n",
    "Multimodal Agent AI systems have many applications. In addition to interactive AI, grounded multimodal models could help drive content generation for bots and AI agents, and assist in productivity applications, helping to re-play, paraphrase, action prediction or synthesize 3D or 2D scenario. Fundamental advances in agent AI help contribute towards these goals and many would benefit from a greater understanding of how to model embodied and empathetic in a simulate reality or a real world. Arguably many of these applications could have positive benefits.\n",
    "\n",
    "However, this technology could also be used by bad actors. Agent AI systems that generate content can be used to manipulate or deceive people. Therefore, it is very important that this technology is developed in accordance with responsible AI guidelines. For example, explicitly communicating to users that content is generated by an AI system and providing the user with controls in order to customize such a system. It is possible the Agent AI could be used to develop new methods to detect manipulative content - partly because it is rich with hallucination performance of large foundation model - and thus help address another real world problem.\n",
    "\n",
    "For examples, 1) in health topic, ethical deployment of LLM and VLM agents, especially in sensitive domains like healthcare, is paramount. AI agents trained on biased data could potentially worsen health disparities by providing inaccurate diagnoses for underrepresented groups. Moreover, the handling of sensitive patient data by AI agents raises significant privacy and confidentiality concerns. 2) In the gaming industry, AI agents could transform the role of developers, shifting their focus from scripting non-player characters to refining agent learning processes. Similarly, adaptive robotic systems could redefine manufacturing roles, necessitating new skill sets rather than replacing human workers. Navigating these transitions responsibly is vital to minimize potential socio-economic disruptions.\n",
    "\n",
    "Furthermore, the agent AI focuses on learning collaboration policy in simulation and there is some risk if directly applying the policy to the real world due to the distribution shift. Robust testing and continual safety monitoring mechanisms should be put in place to minimize risks of unpredictable behaviors in real-world scenarios. Our ‚ÄúVideoAnalytica\" dataset is collected from the Internet and considering which is not a fully representative source, so we already go through-ed the ethical review and legal process from both Microsoft and University Washington. Be that as it may, we also need to understand biases that might exist in this corpus. Data distributions can be characterized in many ways. In this workshop, we have captured how the agent level distribution in our dataset is different from other existing datasets. However, there is much more than could be included in a single dataset or workshop. We would argue that there is a need for more approaches or discussion linked to real tasks or topics and that by making these data or system available.\n",
    "\n",
    "We will dedicate a segment of our project to discussing these ethical issues, exploring potential mitigation strategies, and deploying a responsible multi-modal AI agent. We hope to help more researchers answer these questions together via this paper.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742bba7d",
   "metadata": {},
   "source": [
    "### 2.5 Chunking Document\n",
    "\n",
    "### 2.5.1 Why do we need to chunk the document? \n",
    "\n",
    "\n",
    "In a RAG (Retrieval-Augmented Generation) system, long documents are **split into smaller pieces**, called **chunks**, before being embedded and stored. This process is essential for several reasons:\n",
    "\n",
    "\n",
    "####  1. Context Window Limitations\n",
    "\n",
    "Most embedding models and language models (e.g., GPT) have a maximum input size (called a **context window**), typically around **512 to 4096 tokens**.  \n",
    "If a document is too long, it won't fit ‚Äî so we must split it into smaller parts.\n",
    "\n",
    "\n",
    "####  2. Improved Retrieval Accuracy\n",
    "\n",
    "When a query is made, the retriever searches for **the most relevant chunks**, not entire documents.  \n",
    "Smaller, focused chunks help the model retrieve **more precise** and **contextually relevant** information.\n",
    "\n",
    "- Too large: one irrelevant section may dominate the similarity score  \n",
    "- Too small: might lose meaning or break context\n",
    "\n",
    "\n",
    "####  3. Embedding Models Perform Better on Shorter Text\n",
    "\n",
    "Most embedding models (like `text-embedding-ada-002` or `MiniLM`) are trained to represent **short text units**, such as sentences or paragraphs.  \n",
    "Chunking ensures we stay within the range the model was optimized for.\n",
    "\n",
    "\n",
    "####  4. Flexibility in Retrieval Granularity\n",
    "\n",
    "Chunked documents allow the system to:\n",
    "- Retrieve only the relevant **subsections**\n",
    "- Combine multiple chunks from different sources\n",
    "- Avoid returning large blocks of irrelevant content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e65ff4",
   "metadata": {},
   "source": [
    "Here we simpily splits the document into sequential slices of 150 characters, regardless of word or sentence boundaries.\n",
    "\n",
    "\n",
    "#### Other options strategy for chunking\n",
    "\n",
    "| Strategy           | When to Use                          |\n",
    "| ------------------ | ------------------------------------ |\n",
    "| Fixed-size cutting | Fast prototype, basic use cases      |\n",
    "| Sentence-based     | Better for language tasks & meaning  |\n",
    "| Sliding window     | When continuity/context is important |\n",
    "| Semantic chunking  | For high-quality production systems  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f44aa",
   "metadata": {},
   "source": [
    "\n",
    "### 2.5.2 Does the size matter (why `chunk_size= 150` here ?)\n",
    "\n",
    "#### Chunk size affects :\n",
    "\n",
    "| Aspect                | Impact                                                    |\n",
    "| --------------------- | --------------------------------------------------------- |\n",
    "|  Retrieval quality  | Small chunks = more precise, large chunks = more context  |\n",
    "|  Embedding accuracy | Too long ‚Üí diluted meaning, too short ‚Üí incomplete        |\n",
    "|  Model performance  | LLMs prefer well-formed input (sentences, not fragments)  |\n",
    "|  API efficiency     | Larger chunks = fewer API calls, but more tokens per call |\n",
    "\n",
    "#### Chunk strateguy for different document type \n",
    "\n",
    "| Document Type           | Recommended Chunk Strategy           |\n",
    "| ----------------------- | ------------------------------------ |\n",
    "| Short FAQs, tweets      | 100‚Äì150 characters (1‚Äì2 sentences)   |\n",
    "| Web articles, emails    | 200‚Äì300 characters (2‚Äì3 sentences)   |\n",
    "| Technical docs, papers  | 400‚Äì600 characters or sentence-based |\n",
    "| Legal/Medical documents | Sentence-based or 512‚Äì1024 tokens    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "129d68b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 150 # Try other size !\n",
    "\n",
    "chunks = [embedding_text[i:i + chunk_size] for i in range(0, len(embedding_text), chunk_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb9aafa",
   "metadata": {},
   "source": [
    "### 2.6 Vectorization\n",
    "\n",
    "We vectorize each document and compare the cosine similarity of the vectors to find the document fragment that is closest to the question. Next, we embed these small text blocks to get a 1024-dimensional vector. For vectorization, we need to use the previous emb_model. Then, we store these vectors in a vector database for subsequent retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53141be7",
   "metadata": {},
   "source": [
    "### Q:Why Normalize Embeddings After Chunking?\n",
    "\n",
    "Even if all text chunks are of equal size (e.g., 150 characters), their **embedding magnitudes (vector norms)** can still vary due to differences in content.\n",
    "\n",
    "\n",
    "### Why It Matters\n",
    "\n",
    "- `faiss.IndexFlatIP` computes inner product as similarity.\n",
    "- Without normalization: longer vectors may score higher regardless of meaning.\n",
    "- With normalization: inner product becomes cosine similarity, focusing on semantic similarity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f2a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    response = client.embeddings.create(\n",
    "        model=emb_model,\n",
    "        input=chunk,\n",
    "    )\n",
    "    embeddings.append(response.data[0].embedding)\n",
    "\n",
    "\n",
    "normalized_embeddings = normalize(np.array(embeddings).astype('float32'))\n",
    "\n",
    "d = len(embeddings[0])\n",
    "index = faiss.IndexFlatIP(d) # Create a Faiss index for storing and retrieving embedding vectors\n",
    "index.add(normalized_embeddings) # Add normalized embedding to the Faiss index \n",
    "n_vectors = index.ntotal\n",
    "\n",
    "\n",
    "print(n_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5717414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def match_text(input_text, index, chunks, k=2):\n",
    "    \"\"\"\n",
    "    Given a set of chunks, find the top k chunks that are most similar to the input text.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    input_text (str): Input text to match.\n",
    "    index (faiss.Index): Faiss index to search.\n",
    "    chunks (list of str): List of chunks.\n",
    "    k (int, optional): Number of most similar chunks to return. Default is 2.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    str: Formatted string containing the most similar chunks and their similarity.\n",
    "\n",
    "    \"\"\"\n",
    "    # Make sure K doesn't exceed the total chunks\n",
    "    k = min(k, len(chunks))\n",
    "\n",
    "    \n",
    "    response = client.embeddings.create(\n",
    "        model=emb_model,\n",
    "        input=input_text,\n",
    "    )\n",
    "\n",
    "\n",
    "    input_embedding = response.data[0].embedding\n",
    "    input_embedding = normalize(np.array([input_embedding]).astype('float32'))\n",
    "\n",
    "    # Search the index for the k vectors most similar to the input embedding vector\n",
    "    distances, indices = index.search(input_embedding, k)\n",
    "\n",
    "\n",
    "    matching_texts = \"\"\n",
    "\n",
    "    for i, idx in enumerate(indices[0]): \n",
    "        # Print every similar text\n",
    "        print(f\"similarity: {distances[0][i]:.4f}\\nmatching text: \\n{chunks[idx]}\\n\")\n",
    "        # Add similarity and text content to the matching text string\n",
    "        matching_texts += f\"similarity: {distances[0][i]:.4f}\\nmatching text: \\n{chunks[idx]}\\n\"\n",
    "\n",
    "\n",
    "    return matching_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27fe4c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity: 0.6790\n",
      "matching text: \n",
      "\n",
      "Multimodal Agent AI systems have many applications. In addition to interactive AI, grounded multimodal models could help drive content generation for\n",
      "\n",
      "similarity: 0.5948\n",
      "matching text: \n",
      "ystem and providing the user with controls in order to customize such a system. It is possible the Agent AI could be used to develop new methods to de\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What are the applications of Agent AI systems ?\"\n",
    "\n",
    "matched_texts = match_text(input_text=input_text, index=index, chunks=chunks, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280893e9",
   "metadata": {},
   "source": [
    "We can see that the matching text is not complete, which is related to the chunking method, but it does not affect the result because the model is generated based on this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d09d8c",
   "metadata": {},
   "source": [
    "###  Construct questioning prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c87efe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "According to the documents found\n",
    "{matched_texts}\n",
    "generate\n",
    "{input_text}\n",
    "When answering questions, use the original text of the document if possible. Do not restate the question, just start answering it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6ca7b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_stream(prompt):\n",
    "    \"\"\"\n",
    "    Generates a streaming text reply using OpenAI's Chat Completions API.\n",
    "\n",
    "    Parameters:\n",
    "    prompt (str): The prompt text to generate the reply.\n",
    "\n",
    "    Returns:\n",
    "    None: This function directly prints the generated reply content.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=chat_model,  \n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    if response:\n",
    "        \n",
    "        for chunk in response:\n",
    "            \n",
    "            content = chunk.choices[0].delta.content\n",
    "            \n",
    "            if content:\n",
    "                \n",
    "                print(content, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "003316d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal Agent AI systems have many applications. In addition to interactive AI, grounded multimodal models could help drive content generation."
     ]
    }
   ],
   "source": [
    "get_completion_stream(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

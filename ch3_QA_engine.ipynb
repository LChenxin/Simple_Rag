{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc749b3a",
   "metadata": {},
   "source": [
    "# Chapter 3: Building Your First Question-Answering Engine\n",
    "\n",
    "\n",
    ">This notebook is based on the open-source project [wow-rag](https://github.com/datawhalechina/wow-rag) by Datawhale China.  \n",
    ">I’ve adapted and annotated parts of it for personal learning and experimentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8276143e",
   "metadata": {},
   "source": [
    "## 1. Introduction \n",
    "\n",
    "\n",
    "In this chapter, we take a hands-on approach to building a functional **Retrieval-Augmented Generation (RAG)** pipeline by implementing four distinct methods to construct a question-answering engine.\n",
    "\n",
    "Each method demonstrates a different level of control, flexibility, and scalability, helping you understand the design space of modern QA systems.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Explore Multiple Methods?\n",
    "\n",
    "RAG systems typically consist of three main components:\n",
    "1. **Document Ingestion & Indexing**\n",
    "2. **Semantic Retrieval**\n",
    "3. **Answer Generation via LLMs**\n",
    "\n",
    "While the basic pipeline might seem straightforward, the implementation details—such as how documents are parsed, where vectors are stored, and how queries are synthesized—can significantly impact performance and usability. Exploring multiple methods allows you to:\n",
    "\n",
    "-  Compare ease of use vs. customizability\n",
    "-  Evaluate performance across different storage backends (in-memory vs. vector DB)\n",
    "-  Understand how preprocessing affects search quality\n",
    "-  Learn how to modularize and scale your RAG system\n",
    "\n",
    "---\n",
    "\n",
    "### Overview of the Four Methods\n",
    "\n",
    "| Method | Description | Use Case |\n",
    "|--------|-------------|----------|\n",
    "| **Method 1** | Use `VectorStoreIndex` to build an index directly from documents | Best for quick prototyping and minimal setup |\n",
    "| **Method 2** | Split documents into `nodes` using `SentenceSplitter`, then build a custom FAISS index | Offers more control over chunking and indexing, improving semantic retrieval |\n",
    "| **Method 3** | Construct custom `Retriever` + `ResponseSynthesizer` components, then bind into a `QueryEngine` | Suitable for fine-tuning retrieval and generation separately |\n",
    "| **Method 4** | Use an external **vector DB (Qdrant)** with file-based ingestion, retriever, synthesizer, and metadata filters | Best for production-scale systems with large datasets and advanced filtering needs |\n",
    "\n",
    "\n",
    "### What We Will Learn\n",
    "\n",
    "By the end of this chapter, we will be able to:\n",
    "\n",
    "- Construct RAG pipelines with increasing complexity\n",
    "- Choose between local or external vector storage backends\n",
    "- Preprocess documents for better retrieval performance\n",
    "- Combine custom components to optimize the user’s question-answering experience\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c5a2d",
   "metadata": {},
   "source": [
    "## 2. Preparation\n",
    "\n",
    "At the end of the previous chapter, we proved that the embedding performance of OpenAI's model is generally better than that of the local model, so we directly call the OpenAI model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f49444af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "\n",
    "base_url = \"https://api.openai.com/v1\"  \n",
    "chat_model = \"gpt-4.1-nano-2025-04-14\"   \n",
    "emb_model = \"text-embedding-3-small\"\n",
    "\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(\n",
    "    api_key = api_key,\n",
    "    model = chat_model,\n",
    ")\n",
    "\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "embedding = OpenAIEmbedding(\n",
    "    api_key = api_key,\n",
    "    model = emb_model,\n",
    ")\n",
    "\n",
    "emb = embedding.get_text_embedding(\"Hello\")\n",
    "len(emb) # Output 1536 if working"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef28b4a2",
   "metadata": {},
   "source": [
    "Before everything starts, we need to prepare a document for this part, for example we prepared a example.txt in the  ```./docs/example.txt ```\n",
    "\n",
    "\n",
    "it still the same excerpt with **Chapter 1**  From [arXiv:2401.03568](https://ar5iv.labs.arxiv.org/html/2401.03568):\n",
    "\n",
    "In order to to have a grasp of the the search capabilities, I recommend that readers replace the `example.txt` with an article they are familiar with, or at least read it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f37e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the specified file, input is List\n",
    "from llama_index.core import SimpleDirectoryReader,Document\n",
    "documents = SimpleDirectoryReader(input_files=['./docs/example.txt']).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7f333",
   "metadata": {},
   "source": [
    "## 3. Method 1:  Build  index directly from documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b83911",
   "metadata": {},
   "source": [
    "### 3.1 Build semantic vector index \n",
    "\n",
    "By using `LlamaIndex`, we can build a *semantic vector index* from a list of documents .\n",
    "\n",
    "this line performs:\n",
    "``` Documents → Text chunks → Embedding vectors → Indexed into a vector store ```\n",
    "\n",
    "This is equivalent to what we manually did in **Lesson 1 (Hack-a-RAG)**:\n",
    "\n",
    "-  Manually chunking the text\n",
    "\n",
    "- Calling embedding API\n",
    "\n",
    "-  Normalizing + storing in FAISS\n",
    "\n",
    "But now it's all automated in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b076be42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1b37d00d554cd5b00b14e20d943064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d375cb69193e4331b5ba2501d22215cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex.from_documents(documents,embed_model=embedding,show_progress=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e29ad",
   "metadata": {},
   "source": [
    "### 3.2 Build Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c6c581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI refers to systems that can perform tasks typically requiring human intelligence, such as content generation, action prediction, and scenario synthesis.\n",
      "These systems can be embodied and empathetic, operating in simulated or real environments, and are used across various applications like interactive agents, healthcare, gaming, and manufacturing.\n",
      "Responsible development and deployment are essential to address ethical concerns, privacy, bias, and safety issues associated with AI technologies.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(llm=llm)\n",
    "# Start answer the question \n",
    "response = query_engine.query(\"what is AI？\")\n",
    "\n",
    "raw_answer = response.response\n",
    "\n",
    "# Automatically add newline after each period (optionally followed by a space)\n",
    "formatted_answer = raw_answer.replace('. ', '.\\n')\n",
    "\n",
    "# Print the result with line breaks\n",
    "print(formatted_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761fb4b",
   "metadata": {},
   "source": [
    "#### Note: Key Elements of `response`\n",
    "\n",
    "| Attribute                   | Type                  | Description                                                                 |\n",
    "|-----------------------------|-----------------------|-----------------------------------------------------------------------------|\n",
    "| `response.response`         | `str`                 | The **main answer text** generated by the LLM.                              |\n",
    "| `response.source_nodes`     | `List[NodeWithScore]` | A list of **retrieved document chunks** (nodes) used to form the answer. Each includes metadata, text, and similarity score. |\n",
    "| `response.metadata`         | `Dict`                | Metadata info about the response. Often contains file paths, creation date, etc. |\n",
    "| `response.formatted_sources`| `str` (optional)      | A preformatted string of sources, if enabled in config.                     |\n",
    "| `response.extra_info`       | `Dict` or `None`      | Any additional information added during processing, e.g., confidence scores, reranking data. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb06d5a",
   "metadata": {},
   "source": [
    "#### ✅ When to Use This Method\n",
    "\n",
    "- You want a **quick and simple** way to build a RAG pipeline.\n",
    "- Your dataset is **small to medium-sized** and doesn't require fine-tuned control.\n",
    "- You're in a **prototype or experimentation** phase and want fast iteration.\n",
    "- You prefer to let `LlamaIndex` handle chunking, embedding, and indexing for you.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🚫 When *Not* to Use This Method\n",
    "\n",
    "- You need **custom chunking logic** (e.g., sentence-aware splits, metadata injection).\n",
    "- You want to use **external vector stores** like Qdrant, Pinecone, or Weaviate.\n",
    "- You’re preparing for **production deployment** and need full control over indexing and retrieval.\n",
    "- Your documents contain **structured content** (e.g., tables, metadata) that require preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "This method is perfect for fast onboarding, but you’ll eventually need to explore more modular approaches (like the ones in Method 2 and beyond) to scale and optimize performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa5712d",
   "metadata": {},
   "source": [
    "## 4. Method 2: Custom index with FAISS\n",
    "\n",
    "Here we will split documents into `nodes` using `SentenceSplitter`, then build a custom FAISS index these will offers more control over chunking and indexing, improving semantic retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd5073d",
   "metadata": {},
   "source": [
    "#### Step-by-Step Breakdown\n",
    "\n",
    " 1. `SentenceSplitter(chunk_size=512)`\n",
    "\n",
    "- A **node parser** that breaks long text into smaller **chunks (nodes)**.\n",
    "- It tries to preserve **sentence boundaries** when splitting.\n",
    "- Each resulting node contains approximately **512 characters**.\n",
    "- This improves embedding quality by keeping **semantic units together**.\n",
    "\n",
    "---\n",
    "\n",
    " 2. `transformations = [...]`\n",
    "\n",
    "- A list of **transformation steps** applied to the documents.\n",
    "- You can chain multiple transformations (e.g., **splitting**, **cleaning**, **metadata injection**).\n",
    "- In this example, we only use one: `SentenceSplitter`.\n",
    "\n",
    "---\n",
    "\n",
    " 3. `run_transformations(documents, transformations=...)`\n",
    "\n",
    "- This function **applies all transformations** in order to your list of documents.\n",
    "- The output is a list of **`Node` objects**, each representing a semantically meaningful chunk of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01835e9f",
   "metadata": {},
   "source": [
    "### 4.1 Nodes construction (Chunking) \n",
    "\n",
    "First we will**manually processes documents into nodes (chunks)** using LlamaIndex’s transformation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed625da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split documents into chunks\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "transformations = [SentenceSplitter(chunk_size = 256)]\n",
    "\n",
    "from llama_index.core.ingestion.pipeline import run_transformations\n",
    "nodes = run_transformations(documents, transformations=transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "272c8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Assign stable, manual IDs to each node, By default each node.id used random UUID.\n",
    "for i, node in enumerate(nodes):\n",
    "    node.id_ = str(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb633db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.schema import TextNode  # ✅ updated path\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "\n",
    "# 3. Build FAISS vector store\n",
    "dims = len(embedding.get_text_embedding(\"hello\"))\n",
    "faiss_index = faiss.IndexFlatL2(dims) # Why L2 insted of Cos ?\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f9a5e3",
   "metadata": {},
   "source": [
    "####  Note: why use `IndexFlatL2` L2 insted of  Cosine Similarity\n",
    "\n",
    "- FAISS **does not** have native cosine similarity.\n",
    "- But cosine and L2 behave similarly when **vectors are normalized**.\n",
    "- So many embedding providers normalize vectors, making **L2 ≈ Cosine**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e34f7e",
   "metadata": {},
   "source": [
    "### 4.2 Construct vector index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd32128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create index and insert nodes\n",
    "index = VectorStoreIndex(nodes=[], embed_model=embedding, storage_context=storage_context)\n",
    "index.insert_nodes(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe1d99",
   "metadata": {},
   "source": [
    "### 4.3 Build Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd687451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence (AI) refers to systems and agents that can perform tasks typically requiring human intelligence, such as learning, decision-making, and problem-solving.\n",
      "These systems can be used across various domains, including healthcare, gaming, and manufacturing, to enhance efficiency and innovation.\n",
      "However, their development and deployment require careful consideration of ethical, privacy, and safety concerns to prevent misuse and address potential societal impacts.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(llm=llm)\n",
    "response = query_engine.query(\"what is AI？\")\n",
    "\n",
    "raw_answer = response.response\n",
    "formatted_answer = raw_answer.replace('. ', '.\\n')\n",
    "print(formatted_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e597520",
   "metadata": {},
   "source": [
    "### 4.4 Save the Index\n",
    "\n",
    "We can also save the index locally to save the need to calculate it every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12a0f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index to disk\n",
    "persist_dir = \"./storage\"\n",
    "index.storage_context.persist(persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68050fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage\\docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage\\index_store.json.\n"
     ]
    }
   ],
   "source": [
    "# load index from disk\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "vector_store = FaissVectorStore.from_persist_dir(persist_dir)\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=persist_dir\n",
    ")\n",
    "index = load_index_from_storage(storage_context=storage_context,embed_model = embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf96d65",
   "metadata": {},
   "source": [
    "Reload the index and use the same query, we should get the same reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eecd52cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence (AI) refers to systems and agents that can perform tasks typically requiring human intelligence, such as learning, decision-making, and problem-solving.\n",
      "These systems can be used in various domains, including healthcare, gaming, and manufacturing, to enhance efficiency and innovation.\n",
      "However, their development and deployment require careful consideration of ethical, privacy, and safety concerns to prevent misuse and ensure beneficial outcomes.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(llm=llm)\n",
    "response = query_engine.query(\"what is AI？\")\n",
    "\n",
    "raw_answer = response.response\n",
    "formatted_answer = raw_answer.replace('. ', '.\\n')\n",
    "print(formatted_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8f6946",
   "metadata": {},
   "source": [
    "## 5. Method 3: Custom Retriever + ResponseSynthesizer \n",
    "\n",
    "In this method, we manually build the core components of a RAG pipeline using `LlamaIndex`:\n",
    "\n",
    "- A **Retriever**: Responsible for fetching top-k semantically similar chunks from the index.\n",
    "- A **ResponseSynthesizer**: Responsible for generating a final answer from the retrieved chunks using the LLM.\n",
    "- A **RetrieverQueryEngine**: Combines both components to form a flexible and extensible query engine.\n",
    "\n",
    "---\n",
    "\n",
    "###  Why use this method?\n",
    "\n",
    "- Gives you **more granular control** over the retrieval and generation pipeline.\n",
    "- Allows you to **fine-tune parameters** (e.g., number of retrieved documents, LLM behavior).\n",
    "- Enables **experimentation and customization**, such as:\n",
    "  - Using different LLMs for retrieval and response.\n",
    "  - Adding reranking, metadata filtering, or summarization.\n",
    "\n",
    "---\n",
    "\n",
    "###  When is it useful?\n",
    "\n",
    "- When you're moving beyond prototypes and need **production-ready pipelines**.\n",
    "- When you want to **optimize retrieval quality and generation separately**.\n",
    "- When integrating into **larger systems** with multiple components or data sources.\n",
    "\n",
    "---\n",
    "\n",
    "This method offers a balanced mix of automation and flexibility, and it's a great stepping stone toward building more advanced and modular RAG systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9099a359",
   "metadata": {},
   "source": [
    "### 5.1 Construct Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "832afbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "\n",
    "kwargs = {'similarity_top_k': 5, 'index': index, 'dimensions': len(emb)} \n",
    "retriever = VectorIndexRetriever(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d13e95",
   "metadata": {},
   "source": [
    "### 5.2 Construct Reponse Synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e973779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers  import get_response_synthesizer\n",
    "response_synthesizer = get_response_synthesizer(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f834e",
   "metadata": {},
   "source": [
    "### 5.3 Construct Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "599ff4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "engine = RetrieverQueryEngine(\n",
    "      retriever=retriever,\n",
    "      response_synthesizer=response_synthesizer\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff5334f",
   "metadata": {},
   "source": [
    "### 5.4 GO ！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25360139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence (AI) refers to systems and technologies that enable machines to perform tasks that typically require human intelligence.\n",
      "These include understanding language, recognizing images, making decisions, and learning from data.\n",
      "AI can be applied across various fields such as healthcare, gaming, manufacturing, and content generation, with a focus on developing responsible and ethical deployment practices to address potential risks like bias, privacy concerns, and manipulation.\n"
     ]
    }
   ],
   "source": [
    "question = \"what is AI？\"\n",
    "answer = engine.query(question)\n",
    "\n",
    "formatted_answer = answer.response.replace('. ', '.\\n')\n",
    "print(formatted_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761cb083",
   "metadata": {},
   "source": [
    "### 5.5 Summary\n",
    "✅ When to Use\n",
    "\n",
    "- You want **full control** over the retrieval and response synthesis process.\n",
    "- You need to **fine-tune retrieval parameters**, such as `similarity_top_k`, distance metrics, or filtering strategies.\n",
    "- You're working in a **modular or production setting**, where components (retriever, generator) may change independently.\n",
    "- You're experimenting with **multi-stage pipelines**, such as reranking or hybrid retrieval.\n",
    "- You need to **integrate metadata filtering**, custom prompt templates, or advanced LLM behaviors.\n",
    "---\n",
    "\n",
    "ℹ️ **Tip**: You can always start with Method 1 (simple) and migrate to Method 3 when your use case demands more flexibility or optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9713ec",
   "metadata": {},
   "source": [
    "##  6. Method 4 : Using a Vector Database (Qdrant) for Scalable Retrieval and Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe84f1d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this section, we explore how to use **Qdrant**, a powerful open-source vector database, as the backend for vector indexing and retrieval.\n",
    "\n",
    "Unlike previous in-memory approaches (e.g., FAISS), Qdrant supports **persistent storage**, **advanced metadata filtering**, and **production-level performance**.\n",
    "\n",
    "We’ll walk through how to:\n",
    "\n",
    "- Load documents and create vector indexes backed by Qdrant\n",
    "- Use the index for semantic search\n",
    "- Enhance retrieval with structured metadata filters\n",
    "- Perform complex logical filtering with Qdrant's native API\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9269ad00",
   "metadata": {},
   "source": [
    "### 6.1 Package installation \n",
    "\n",
    "In this section we need some additional libries: \n",
    "\n",
    "1. **qdrant-client**  \n",
    "   The official Python client for Qdrant, a high-performance vector database.  \n",
    "   - Enables connection to Qdrant (local or cloud).\n",
    "   - Allows uploading vectors, managing collections.\n",
    "   - Supports running similarity searches.\n",
    "\n",
    "2. **llama-index-vector-stores-qdrant**  \n",
    "   Adds Qdrant support to LlamaIndex.  \n",
    "   - Lets you use Qdrant as the backend vector store.\n",
    "   - Useful for semantic search in a RAG pipeline.\n",
    "\n",
    "3. **llama-index-readers-file**  \n",
    "   Provides file reading capabilities for local document ingestion.  \n",
    "   - Allows loading of `.txt`, `.md`, `.pdf`, etc.\n",
    "   - Uses tools like `SimpleDirectoryReader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65b078a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qdrant-client in c:\\users\\liuch\\miniconda3\\lib\\site-packages (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from qdrant-client) (1.73.1)\n",
      "Requirement already satisfied: httpx[http2]>=0.20.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from qdrant-client) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from qdrant-client) (1.25.2)\n",
      "Requirement already satisfied: portalocker<4.0,>=2.7.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from qdrant-client) (3.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from qdrant-client) (6.31.1)\n",
      "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from qdrant-client) (2.11.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from qdrant-client) (1.26.16)\n",
      "Requirement already satisfied: anyio in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (3.7.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (3.4)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from portalocker<4.0,>=2.7.0->qdrant-client) (306)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.1)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from anyio->httpx[http2]>=0.20.0->qdrant-client) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index-vector-stores-qdrant in c:\\users\\liuch\\miniconda3\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: grpcio<2,>=1.60.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-vector-stores-qdrant) (1.73.1)\n",
      "Requirement already satisfied: llama-index-core<0.13,>=0.12.7 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-vector-stores-qdrant) (0.12.49)\n",
      "Requirement already satisfied: qdrant-client>=1.7.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-vector-stores-qdrant) (1.15.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (3.12.14)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.0.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (2.1.3)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (2023.6.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (1.1.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (3.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (1.25.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (9.4.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy[asyncio]>=1.4.49 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (2.0.41)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (4.14.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (1.17.2)\n",
      "Requirement already satisfied: portalocker<4.0,>=2.7.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (3.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (6.31.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (1.26.16)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (1.20.1)\n",
      "Requirement already satisfied: griffe in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (1.7.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (3.1.2)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (3.9.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpx->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (3.7.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpx->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpx->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpx->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (3.4)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpx->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (4.2.0)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (0.16.0)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (0.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (2022.7.9)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from portalocker<4.0,>=2.7.0->qdrant-client>=1.7.1->llama-index-vector-stores-qdrant) (306)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (2.0.4)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (3.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (3.26.1)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from h2<5,>=3->httpx->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from h2<5,>=3->httpx->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (4.1.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (23.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from anyio->httpx->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.7->llama-index-vector-stores-qdrant) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: llama-index-readers-file in c:\\users\\liuch\\miniconda3\\lib\\site-packages (0.4.11)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-readers-file) (4.13.4)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-readers-file) (0.7.1)\n",
      "Requirement already satisfied: llama-index-core<0.13,>=0.12.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-readers-file) (0.12.49)\n",
      "Requirement already satisfied: pandas<2.3.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-readers-file) (2.0.3)\n",
      "Requirement already satisfied: pypdf<6,>=5.1.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-readers-file) (5.8.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-readers-file) (0.0.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file) (2.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file) (4.14.1)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (3.12.14)\n",
      "Requirement already satisfied: aiosqlite in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.0.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (2.1.3)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (2023.6.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (1.1.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (3.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (1.25.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (9.4.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy[asyncio]>=1.4.49 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (2.0.41)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (4.67.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (1.17.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (1.20.1)\n",
      "Requirement already satisfied: griffe in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (1.7.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (3.1.2)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (3.9.1)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (0.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (2022.7.9)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (2023.7.22)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (3.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (3.26.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpx->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (0.16.0)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (23.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from anyio->httpx->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\liuch\\miniconda3\\lib\\site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.0->llama-index-readers-file) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "%pip install qdrant-client\n",
    "%pip install llama-index-vector-stores-qdrant\n",
    "%pip install llama-index-readers-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776163f",
   "metadata": {},
   "source": [
    "### 6.2 Load document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5411d66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 7717dc8a-7445-4cf2-a630-1923a9000cfa\n"
     ]
    }
   ],
   "source": [
    "import qdrant_client\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=['./docs/example.txt']\n",
    ").load_data()\n",
    "\n",
    "print(\"Document ID:\", documents[0].doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aecb99",
   "metadata": {},
   "source": [
    "### 6.3 Construct index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87dd74d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liuch\\miniconda3\\Lib\\site-packages\\llama_index\\vector_stores\\qdrant\\base.py:709: UserWarning: Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n",
      "  self._client.create_payload_index(\n"
     ]
    }
   ],
   "source": [
    "# Create an index over the documents\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "# Initializes a Qdrant client using a local directory named \"qdrant\" for storage.\n",
    "qclient = qdrant_client.QdrantClient(path=\"qdrant\") \n",
    "# Configure the Vector Store, Think of this as a table or namespace in a database\n",
    "vector_store = QdrantVectorStore(client=qclient, collection_name=\"QA\")\n",
    "# Creates a storage configuration that binds the index to your Qdrant vector store\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# Build the Vector Index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    storage_context=storage_context,\n",
    "    embed_model = embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3460659",
   "metadata": {},
   "source": [
    "### 6.4 Construct Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48e65d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "kwargs = {'similarity_top_k': 5, 'index': index, 'dimensions': len(emb)} \n",
    "retriever = VectorIndexRetriever(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866f552",
   "metadata": {},
   "source": [
    "### 6.5 Construct Reponse synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf66c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.response_synthesizers  import get_response_synthesizer\n",
    "response_synthesizer = get_response_synthesizer(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b86338d",
   "metadata": {},
   "source": [
    "### 6.6 Construct Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a930b723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI, or Artificial Intelligence, refers to systems and agents designed to perform tasks that typically require human intelligence.\n",
      "These include content generation, decision-making, learning collaboration policies, and adaptive behaviors across various domains such as healthcare, gaming, manufacturing, and content creation.\n",
      "AI systems can be multimodal, integrating different types of data like visual and textual information, and are developed to model embodied and empathetic interactions in both simulated and real-world environments.\n",
      "Responsible development and deployment of AI emphasize transparency, privacy, and minimizing biases to ensure positive societal impacts.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is AI ?\"\n",
    "answer = engine.query(question)\n",
    "\n",
    "formatted_answer = answer.response.replace('. ', '.\\n')\n",
    "print(formatted_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7663d",
   "metadata": {},
   "source": [
    "### 6.7 Defining Custom Text Nodes with Metadata for Filtering\n",
    "\n",
    "In this subsection, we demonstrate how to use **metadata filters** to perform semantic retrieval **only on a subset of nodes** that meet certain criteria.\n",
    "\n",
    "####  What we are doing:\n",
    "\n",
    "1. **Manually construct `TextNode` objects**, each with:\n",
    "   - A text string (e.g. a movie title or summary)\n",
    "   - Associated metadata (e.g., author, theme, year)\n",
    "\n",
    "2. **Create a `QdrantVectorStore`** and store the `TextNode`s with embeddings.\n",
    "\n",
    "3. Use `MetadataFilters` to **limit retrieval** to only those nodes where:\n",
    "   - The `\"theme\"` field is equal to `\"Mafia\"`.\n",
    "\n",
    "4. Call `retriever.retrieve(\"What is inception about?\")` to:\n",
    "   - Embed the query\n",
    "   - Search **only among documents with `\"theme\": \"Mafia\"`** for relevant results\n",
    "   - Return a list of semantically similar `TextNode`s\n",
    "\n",
    "#### 🎯 Why we do this:\n",
    "\n",
    "- In real-world applications (e.g., RAG systems in production), you often want to **narrow down the search scope** before doing vector similarity search.\n",
    "- For example, only retrieve:\n",
    "  - Documents from a specific department or time period\n",
    "  - Articles written by a specific author\n",
    "  - Logs tagged with a specific error type\n",
    "\n",
    "- **Metadata filtering allows you to combine symbolic rules with semantic retrieval**, improving both accuracy and efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "415b0091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "nodes = [\n",
    "    TextNode(\n",
    "        text=\"The Shawshank Redemption\",\n",
    "        metadata={\n",
    "            \"author\": \"Stephen King\",\n",
    "            \"theme\": \"Friendship\",\n",
    "            \"year\": 1994,\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"The Godfather\",\n",
    "        metadata={\n",
    "            \"director\": \"Francis Ford Coppola\",\n",
    "            \"theme\": \"Mafia\",\n",
    "            \"year\": 1972,\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"Inception\",\n",
    "        metadata={\n",
    "            \"director\": \"Christopher Nolan\",\n",
    "            \"theme\": \"Fiction\",\n",
    "            \"year\": 2010,\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"To Kill a Mockingbird\",\n",
    "        metadata={\n",
    "            \"author\": \"Harper Lee\",\n",
    "            \"theme\": \"Mafia\",\n",
    "            \"year\": 1960,\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"1984\",\n",
    "        metadata={\n",
    "            \"author\": \"George Orwell\",\n",
    "            \"theme\": \"Totalitarianism\",\n",
    "            \"year\": 1949,\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"The Great Gatsby\",\n",
    "        metadata={\n",
    "            \"author\": \"F. Scott Fitzgerald\",\n",
    "            \"theme\": \"The American Dream\",\n",
    "            \"year\": 1925,\n",
    "        },\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"Harry Potter and the Sorcerer's Stone\",\n",
    "        metadata={\n",
    "            \"author\": \"J.K. Rowling\",\n",
    "            \"theme\": \"Fiction\",\n",
    "            \"year\": 1997,\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a36b8",
   "metadata": {},
   "source": [
    "### 6.8 Construct index base on the Custom Text Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2dd9e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(client=qclient, collection_name=\"filter\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex(\n",
    "    nodes, \n",
    "    storage_context=storage_context,\n",
    "    embed_model = embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b846e13",
   "metadata": {},
   "source": [
    "### 6.9 Construct metadata filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72061d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    "    FilterOperator,\n",
    ")\n",
    "\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(key=\"theme\", operator=FilterOperator.EQ, value=\"Mafia\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a8566",
   "metadata": {},
   "source": [
    "### 6.10 Construct retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5713956c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='6e6ffe25-ef1a-40ac-81fb-436e8eeabf7d', embedding=None, metadata={'director': 'Francis Ford Coppola', 'theme': 'Mafia', 'year': 1972}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='The Godfather', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.1802265105404427)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = index.as_retriever(filters=filters, llm=llm)\n",
    "retriever.retrieve(\"What is inception about?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a088e1",
   "metadata": {},
   "source": [
    "**Result explanation**\n",
    "Only nodes that match the metadata condition are included in the search space.\n",
    "\n",
    "Among those, the semantic similarity score determines which one is returned.\n",
    "\n",
    "Since only \"The Godfather\" matched the theme: Mafia tag, it was returned—even though it’s semantically unrelated to the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e1e36c",
   "metadata": {},
   "source": [
    "### 6.11 Advanced metadata filter\n",
    "\n",
    "We can also combine multiple filters using AND or OR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c4030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='d83c4f7e-951b-486a-b19e-407ccfbbfbf4', embedding=None, metadata={'director': 'Christopher Nolan', 'theme': 'Fiction', 'year': 2010}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Inception', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.19519303978658628)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.vector_stores import FilterOperator, FilterCondition\n",
    "\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(key=\"theme\", value=\"Fiction\"),\n",
    "        MetadataFilter(key=\"year\", value=1997, operator=FilterOperator.GT), # Notice that there is a operator GT stand for \">\"\n",
    "    ],\n",
    "    condition=FilterCondition.AND,\n",
    ")\n",
    "\n",
    "retriever = index.as_retriever(filters=filters, llm=llm)\n",
    "retriever.retrieve(\"Harry Potter?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00fba1",
   "metadata": {},
   "source": [
    "We can also directly use the filter dictionary as a parameter to construct a retriever, which can build a more complex filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df20b3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='d83c4f7e-951b-486a-b19e-407ccfbbfbf4', embedding=None, metadata={'director': 'Christopher Nolan', 'theme': 'Fiction', 'year': 2010}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Inception', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.4061743952578851),\n",
       " NodeWithScore(node=TextNode(id_='602f9373-ace2-45df-9508-ebd1062284bd', embedding=None, metadata={'author': 'F. Scott Fitzgerald', 'theme': 'The American Dream', 'year': 1925}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='The Great Gatsby', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.19638923898952426)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = index.as_retriever(\n",
    "    vector_store_kwargs={\"filter\": {\"theme\": \"Mafia\"}},\n",
    "    llm=llm\n",
    ")\n",
    "retriever.retrieve(\"What is inception about?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35b56ac",
   "metadata": {},
   "source": [
    "### 6.12 Using Qdrant's Native Filtering Capabilities with `llama-index`\n",
    "\n",
    "In addition to `llama-index'`s built-in filtering and retrieval pipeline, you can also directly leverage Qdrant's native vector store filtering capabilities. This can be useful when:\n",
    "\n",
    "- You want to use complex filtering logic, including nested conditions.\n",
    "\n",
    "- You want to offload filtering computation to Qdrant, improving performance on large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded536b2",
   "metadata": {},
   "source": [
    "In addition to the search methods provided by llama-index, we can also use Qdrant's own search capabilities. That is, Default Qdrant Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0217ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [\n",
    "    TextNode(\n",
    "        text=\"りんごとは\",\n",
    "        metadata={\"author\": \"Tanaka\", \"fruit\": \"apple\", \"city\": \"Tokyo\"},\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"Was ist Apfel?\",\n",
    "        metadata={\"author\": \"David\", \"fruit\": \"apple\", \"city\": \"Berlin\"},\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"Orange like the sun\",\n",
    "        metadata={\"author\": \"Jane\", \"fruit\": \"orange\", \"city\": \"Hong Kong\"},\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"Grape is...\",\n",
    "        metadata={\"author\": \"Jane\", \"fruit\": \"grape\", \"city\": \"Hong Kong\"},\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"T-dot > G-dot\",\n",
    "        metadata={\"author\": \"George\", \"fruit\": \"grape\", \"city\": \"Toronto\"},\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"6ix Watermelons\",\n",
    "        metadata={\n",
    "            \"author\": \"George\",\n",
    "            \"fruit\": \"watermelon\",\n",
    "            \"city\": \"Toronto\",\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "23574929",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(client=qclient, collection_name=\"default\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex(\n",
    "    nodes, \n",
    "    storage_context=storage_context,\n",
    "    embed_model = embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2da4cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http.models import Filter, FieldCondition, MatchValue\n",
    "filters = Filter(\n",
    "    should=[\n",
    "        Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"fruit\",\n",
    "                    match=MatchValue(value=\"apple\"),\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"city\",\n",
    "                    match=MatchValue(value=\"Tokyo\"),\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        Filter(\n",
    "            must=[\n",
    "                FieldCondition(\n",
    "                    key=\"fruit\",\n",
    "                    match=MatchValue(value=\"grape\"),\n",
    "                ),\n",
    "                FieldCondition(\n",
    "                    key=\"city\",\n",
    "                    match=MatchValue(value=\"Toronto\"),\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0aeee9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(\n",
    "    vector_store_kwargs={\"qdrant_filters\": filters},\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5671ac91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node 0.4104185504365517\n",
      "node T-dot > G-dot\n",
      "node {'author': 'George', 'fruit': 'grape', 'city': 'Toronto'}\n",
      "node 0.2518488836316279\n",
      "node りんごとは\n",
      "node {'author': 'Tanaka', 'fruit': 'apple', 'city': 'Tokyo'}\n"
     ]
    }
   ],
   "source": [
    "response = retriever.retrieve(\"Who makes grapes?\")\n",
    "for node in response:\n",
    "    print(\"node\", node.score)\n",
    "    print(\"node\", node.text)\n",
    "    print(\"node\", node.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c507de7f",
   "metadata": {},
   "source": [
    "Two nodes matched the filters.\n",
    "\n",
    "Among them, \"T-dot > G-dot\" scored highest, since it's semantically closer to the query \"Who makes grapes?\"\n",
    "\n",
    "\"りんごとは\" also matched the filter (apple + Tokyo), but was less relevant semantically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0a805b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This method is ideal when:\n",
    "- You need scalable vector search with **persistent storage**\n",
    "- You want to **filter results based on metadata** (e.g., category, author, language, year)\n",
    "- You're building applications that require **flexible and fast querying at scale**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

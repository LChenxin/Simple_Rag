{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9417eee",
   "metadata": {},
   "source": [
    "# Lesson 2 From Stratch to Scalable\n",
    "\n",
    ">This notebook is based on the open-source project [wow-rag](https://github.com/datawhalechina/wow-rag) by Datawhale China.  \n",
    ">I‚Äôve adapted and annotated parts of it for personal learning and experimentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4173063",
   "metadata": {},
   "source": [
    "###  Integration Options for LLMs and Embeddings in LlamaIndex\n",
    "\n",
    "LlamaIndex provides several flexible ways to integrate LLMs and embedding models into your RAG pipeline.  \n",
    "While we‚Äôll start with OpenAI for simplicity, it's helpful to understand the broader ecosystem and future possibilities.\n",
    "\n",
    "#### Available Integration Options:\n",
    "\n",
    "1. **Use official LlamaIndex client packages**  \n",
    "   For providers like ZhipuAI or Yi-34B, LlamaIndex offers pre-built wrappers that make integration seamless.\n",
    "\n",
    "2. **Use OpenAI-compatible APIs**  *(our choice for now)*  \n",
    "   This includes OpenAI's official endpoints and any API that mimics OpenAI's interface (e.g., OpenRouter, Moonshot).  \n",
    "   Since we are using OpenAI directly from abroad, the setup is straightforward and reliable.\n",
    "\n",
    "3. **Use custom model classes**  \n",
    "   LlamaIndex allows advanced users to implement their own `LLM` or `Embedding` classes, offering full control over how the models behave.\n",
    "\n",
    "4. **Use local models via Ollama** *(exploration planned)*  \n",
    "   Ollama allows running open-source models (like LLaMA, Mistral, Gemma) on your own machine with an OpenAI-compatible interface.  \n",
    "   While we won‚Äôt use this in Lesson 2, we plan to explore its potential for offline, privacy-focused, or cost-efficient setups.\n",
    "\n",
    "---\n",
    "\n",
    " In this notebook, we‚Äôll proceed with **Option 2 (OpenAI)** to build a clean, minimal RAG prototype using official APIs.  \n",
    "Later, we may experiment with **Option 4 (Ollama)** to compare local model performance and flexibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d9476",
   "metadata": {},
   "source": [
    "## 1. Introduction to Lamaindex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7159ebc0",
   "metadata": {},
   "source": [
    "###  What is LlamaIndex (formerly GPT Index)?\n",
    "\n",
    "**LlamaIndex** is a Python library designed to **connect your documents to language models** (like GPT) in an efficient, flexible, and scalable way.\n",
    "\n",
    "It simplifies and automates key steps in a **RAG (Retrieval-Augmented Generation)** pipeline, such as:\n",
    "\n",
    "- Document ingestion\n",
    "- Text chunking and indexing\n",
    "- Embedding and storage\n",
    "- Query retrieval\n",
    "- Response generation\n",
    "\n",
    "---\n",
    "\n",
    "###  Why Use LlamaIndex in RAG?\n",
    "\n",
    "RAG systems require multiple components to work together:\n",
    "- Chunking\n",
    "- Embedding\n",
    "- Indexing\n",
    "- Retrieving\n",
    "- Feeding context to an LLM\n",
    "\n",
    "**LlamaIndex wraps all of this into an easy-to-use interface**, so you can build a complete RAG pipeline with minimal boilerplate code.\n",
    "\n",
    "---\n",
    "\n",
    "###  Core Features of LlamaIndex\n",
    "\n",
    "| Feature               | Purpose |\n",
    "|------------------------|---------|\n",
    "| **DocumentLoader**     | Load text from PDFs, websites, files, etc. |\n",
    "| **TextSplitter**       | Automatically chunk text by sentence or tokens |\n",
    "| **VectorStoreIndex**   | Store and search document embeddings |\n",
    "| **QueryEngine**        | Combine retrieved context + LLM to answer questions |\n",
    "| **Storage/Callbacks**  | Persist indexes, log metrics, integrate with LangChain |\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Do I *Need* LlamaIndex?\n",
    "\n",
    "| Situation                         | Recommendation |\n",
    "|----------------------------------|----------------|\n",
    "| Just learning or building from scratch | ‚ùå Not required ‚Äî write your own code (like you did) |\n",
    "| Want fast prototyping / scale up      | ‚úÖ Helpful ‚Äî simplifies multi-step RAG pipelines |\n",
    "| Want to integrate with external data (PDFs, SQL, etc.) | ‚úÖ Strongly recommended |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4671bb0",
   "metadata": {},
   "source": [
    "###  RAG Toolkits: LlamaIndex vs LangChain vs Others\n",
    "\n",
    "When building a RAG pipeline, you can either:\n",
    "- **Write everything from scratch** (manual chunking, embeddings, FAISS, prompt assembly)\n",
    "- OR use **frameworks** that abstract and manage these steps\n",
    "\n",
    "---\n",
    "\n",
    "###  Common RAG Frameworks\n",
    "\n",
    "| Tool/Library    | Description |\n",
    "|-----------------|-------------|\n",
    "| **LlamaIndex**  | Simplifies connecting documents to LLMs (chunking, embedding, querying) |\n",
    "| **LangChain**   | A modular framework for building LLM-powered applications with chains, tools, agents |\n",
    "| **Haystack**    | Enterprise-grade RAG toolkit with Elasticsearch, vector DBs, and pipelines |\n",
    "| **Ragas**       | Focused on **evaluating** RAG systems (not building) |\n",
    "| **PrivateGPT / GPTCache** | For private local inference and response caching |\n",
    "\n",
    "---\n",
    "\n",
    "###  LlamaIndex vs LangChain: Key Differences\n",
    "\n",
    "| Feature              | **LlamaIndex**                         | **LangChain**                                 |\n",
    "|----------------------|----------------------------------------|-----------------------------------------------|\n",
    "| Goal                 | Simple RAG from documents              | General LLM application framework             |\n",
    "| Abstraction Level    | High-level (document ‚Üí answer)         | Mid-level (build your own chains, tools)      |\n",
    "| Focus                | RAG & retrieval                        | Agents, tools, chains, prompts, retrieval     |\n",
    "| Setup                | Easier for beginners                   | More flexible, but steeper learning curve     |\n",
    "| Integration          | Built-in vector store support, OpenAI  | Works with tools, APIs, DBs, vector stores    |\n",
    "| Use Case             | Document Q&A, chat with files          | Complex workflows, agents, multi-step tasks   |\n",
    "\n",
    "---\n",
    "\n",
    "###  When to Use Which?\n",
    "\n",
    "| You want to...                                  | Recommended |\n",
    "|--------------------------------------------------|-------------|\n",
    "| Build simple or academic RAG from documents      | **LlamaIndex** |\n",
    "| Connect LLMs to databases, tools, APIs           | **LangChain** |\n",
    "| Evaluate the quality of a RAG system             | **Ragas** |\n",
    "| Build enterprise-grade, full-stack search        | **Haystack** |\n",
    "| Run LLMs privately without cloud                 | **PrivateGPT** |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654ab40",
   "metadata": {},
   "source": [
    "## 2. Use OpenAI-compatible APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0f106",
   "metadata": {},
   "source": [
    "### 2.1 Install package "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b29d8",
   "metadata": {},
   "source": [
    "####  LlamaIndex Packages Overview\n",
    "\n",
    "This table explains what each installed package does and whether it's necessary when using **OpenAI's embedding and chat models**.\n",
    "\n",
    "| Package Name                              | Purpose / Description                                                | \n",
    "|-------------------------------------------|----------------------------------------------------------------------|\n",
    "| `llama-index-core`                        | Core functionality of LlamaIndex (chunking, indexing, querying)     | \n",
    "| `llama-index-embeddings-zhipuai`          | Embedding model plugin for **ZhipuAI**                               | \n",
    "| `llama-index-llms-zhipuai`                | Chat model plugin for **ZhipuAI**                                    | \n",
    "| `llama-index-embeddings-openai`           | Embedding plugin for **OpenAI** models (e.g., `text-embedding-3`)   | \n",
    "| `llama-index-llms-openai`                 | LLM plugin for **OpenAI** chat models (e.g., `gpt-3.5`, `gpt-4`)     | \n",
    "| `llama-index-readers-file`                | Loader for reading local files (`.txt`, `.md`, `.csv`, etc.)         | \n",
    "| `llama-index-vector-stores-faiss`         | FAISS vector index integration for semantic search                   | \n",
    "| `llamaindex-py-client`                    | Client for accessing **LlamaCloud** API (hosted RAG-as-a-Service)    | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8a40b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-core\n",
    "# %pip install llama-index-embeddings-openai\n",
    "# %pip install llama-index-llms-openai\n",
    "# %pip install llama-index-readers-file\n",
    "# %pip install llama-index-vector-stores-faiss\n",
    "# %pip install llamaindex-py-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ce152",
   "metadata": {},
   "source": [
    "### 2.2 API Configuration and Model Setup\n",
    "\n",
    "same as the first chapter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38c28aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load env\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "\n",
    "base_url = \"hhttps://api.openai.com/v1\"  # We use openai's model here\n",
    "chat_model = \"gpt-4.1-nano-2025-04-14\"   # We will be using cheaper model as im broke AF\n",
    "emb_model = \"text-embedding-3-small\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea30198",
   "metadata": {},
   "source": [
    "### 2.3 Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7758913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(\n",
    "    api_key = api_key,\n",
    "    model = chat_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7fbd66",
   "metadata": {},
   "source": [
    "### 2.4 Model Test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa18ed41",
   "metadata": {},
   "source": [
    "#### üí¨ Why Do We Test `stream_complete()` and `complete()` Separately?\n",
    "\n",
    "In LlamaIndex, both `llm.complete()` and `llm.stream_complete()` are used to generate text from a prompt ‚Äî but they behave differently:\n",
    "\n",
    "---\n",
    "####  `llm.complete(prompt)`\n",
    "- **Returns the full response** as a single object after the whole generation is complete.\n",
    "\n",
    "- Easier for:\n",
    "\n",
    "  - Quick one-off generation\n",
    "\n",
    "  - Logging, string formatting\n",
    "\n",
    "  - Unit tests or offline batch generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab6930d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and assist with a variety of topics. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"Who are youÔºü\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b689c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "####  `llm.stream_complete(prompt)`\n",
    "\n",
    "- **Returns a generator** that yields the response **incrementally**, token by token or chunk by chunk.\n",
    "- Useful for:\n",
    "  - Real-time streaming display\n",
    "  - Responsive chat UI\n",
    "  - Reducing latency in long outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b274b826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Hello\n",
      "\n",
      "Hello!\n",
      "\n",
      "Hello! I\n",
      "\n",
      "Hello! I am\n",
      "\n",
      "Hello! I am Chat\n",
      "\n",
      "Hello! I am ChatGPT\n",
      "\n",
      "Hello! I am ChatGPT,\n",
      "\n",
      "Hello! I am ChatGPT, an\n",
      "\n",
      "Hello! I am ChatGPT, an AI\n",
      "\n",
      "Hello! I am ChatGPT, an AI language\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by Open\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI.\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions,\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information,\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and assist\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and assist with\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and assist with a\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and assist with a variety\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and assist with a variety of\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and assist with a variety of topics\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and assist with a variety of topics.\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and assist with a variety of topics. How\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and assist with a variety of topics. How can\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and assist with a variety of topics. How can I\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and assist with a variety of topics. How can I assist\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and assist with a variety of topics. How can I assist you\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and assist with a variety of topics. How can I assist you today\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and assist with a variety of topics. How can I assist you today?\n",
      "\n",
      "Hello! I am ChatGPT, an AI language model developed by OpenAI. I'm here to help answer your questions, provide information, and assist with a variety of topics. How can I assist you today?"
     ]
    }
   ],
   "source": [
    "response = llm.stream_complete(\"Who are you?\")\n",
    "for chunk in response:\n",
    "    print('\\n')\n",
    "    print(chunk, end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc8af1",
   "metadata": {},
   "source": [
    "### 2.5 Embedding model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee44254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "embedding = OpenAIEmbedding(\n",
    "    api_key = api_key,\n",
    "    model = emb_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded8122c",
   "metadata": {},
   "source": [
    "### 2.6 Test Embedding model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0bbf8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, list)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = embedding.get_text_embedding(\"Hellooooo~\")\n",
    "len(emb), type(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d066d76",
   "metadata": {},
   "source": [
    "Model and also Embedding model are working, both looks great"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cd26a8",
   "metadata": {},
   "source": [
    "## 3. Running locally: Ollama local\n",
    "\n",
    "\n",
    "###  What is Ollama?\n",
    "\n",
    "**Ollama** is a lightweight tool that lets you **run open-source LLMs locally** on your own machine (Mac, Windows, or Linux).  \n",
    "It wraps models like `llama2`, `mistral`, `gemma`, `qwen`, etc., behind a simple API that mimics OpenAI's interface.\n",
    "\n",
    "---\n",
    "\n",
    "####  Key Features of Ollama\n",
    "\n",
    "- Run **chat models locally** without internet or API keys\n",
    "- Use **GPU (if available)** or CPU fallback\n",
    "- Supports multiple models: `llama2`, `mistral`, `gemma`, `qwen`, and more\n",
    "- Comes with a **RESTful API** (OpenAI-compatible) for easy integration\n",
    "- Works with frameworks like **LlamaIndex**, **LangChain**, or even **manual RAG pipelines**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f836411",
   "metadata": {},
   "source": [
    "### 3.1 Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a7a1a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-embeddings-ollama\n",
    "# %pip install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e194d",
   "metadata": {},
   "source": [
    "### 3.2 Talking to Local Models via RESTful API\n",
    "\n",
    "In this section, we will first use the `requests` library to send prompts to a locally running model (e.g., Qwen2 via Ollama) through a RESTful API.\n",
    "\n",
    "---\n",
    "\n",
    "###  What is a RESTful API?\n",
    "\n",
    "A **RESTful API** is a common way for programs to communicate over HTTP using standard methods like:\n",
    "\n",
    "- `GET` ‚Üí retrieve data\n",
    "- `POST` ‚Üí send data (e.g., a user prompt)\n",
    "- `PUT`, `DELETE`, etc.\n",
    "\n",
    "In this case, the **local LLM** (e.g., Ollama) runs a small web server at:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d72be7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model\":\"qwen2:7b\",\"created_at\":\"2025-07-18T15:37:18.2499381Z\",\"message\":{\"role\":\"assistant\",\"content\":\"The\"},\"done\":false}\n",
      "{\"model\":\"qwen2:7b\",\"created_at\":\"2025-07-18T15:37:18.2741113Z\",\"message\":{\"role\":\"assistant\",\"content\":\" rapid\"},\"done\":false}\n",
      "{\"model\":\"qwen2:7b\",\"created_at\":\"2025-07-18T15:37:18.298073Z\",\"message\":{\"role\":\"assistant\",\"content\":\" development\"},\"done\":false}\n",
      "{\"model\":\"qwen2:7b\",\"created_at\":\"2025-07-18T15:37:18.3235007Z\",\"message\":{\"role\":\"assistant\",\"content\":\" and\"},\"done\":false}\n",
      "{\"model\":\"qwen2:7b\",\"created_at\":\"2025-07-18T15:37:18.3491176Z\",\"message\":{\"role\":\"assistant\",\"content\":\" adoption\"},\"done\":false}\n",
      "{\"model\":\"qwen2:7b\",\"created_at\":\"2025-07-18T15:37:18.3742086Z\",\"message\":{\"role\":\"assistant\",\"content\":\" of\"},\"done\":false}\n",
      "{\"model\":\"qwen2:7b\",\"created_at\":\"2025-07-18T15:37:18.4011529Z\",\"message\":{\"role\":\"assistant\",\"content\":\" artificial\"},\"done\":false}\n",
      "{\"model\":\"qwen2:7b\",\"created_at\":\"2025-07-18T15:37:18.4287062Z\",\"message\":{\"role\":\"assistant\",\"cont\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "BASE_URL = \"http://127.0.0.1:11434/api/chat\"\n",
    "\n",
    "\n",
    "payload = {\n",
    "  \"model\": \"qwen2:7b\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Please write an article of about 1,000 words discussing the employment prospects of AI majors.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "response = requests.post(BASE_URL, json=payload)\n",
    "print(response.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4080ec13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As technology continues to advance at an unprecedented pace, the field of artificial intelligence (AI) has rapidly grown and expanded its influence across various industries. The demand for professionals with expertise in this field is on the rise, presenting a promising outlook for graduates specializing in AI.\n",
      "\n",
      "To understand the employment prospects for AI majors, let's first examine what skills these professionals bring to the table:\n",
      "\n",
      "1. **Data Analysis**: AI professionals possess strong analytical abilities and are adept at processing large datasets to uncover patterns and insights that can inform decision-making processes.\n",
      "2. **Machine Learning**: They have a deep understanding of machine learning algorithms, enabling them to create models capable of automating routine tasks and making predictions based on historical data.\n",
      "3. **Programming Skills**: Proficiency in programming languages such as Python, R, and Java is essential for AI professionals, allowing them to develop custom solutions tailored to specific business needs.\n",
      "4. **Problem-solving**: They have exceptional problem-solving abilities, applying creative and logical approaches to tackle complex challenges across various domains.\n",
      "5. **Ethical considerations**: With the increasing use of AI in critical areas like healthcare, finance, and law, an awareness of ethical implications is crucial.\n",
      "\n",
      "With these skills, graduates with a degree in AI or related fields are well-equipped for careers in several sectors:\n",
      "\n",
      "### 1. Technology and Software Development\n",
      "In this domain, AI professionals can work on developing new algorithms, improving existing software systems, or creating innovative applications that leverage machine learning capabilities to provide competitive advantages.\n",
      "\n",
      "For example, they might contribute to projects involving predictive analytics, natural language processing (NLP), computer vision, or robotics.\n",
      "\n",
      "### 2. Healthcare and Biotech\n",
      "AI plays a pivotal role in healthcare, offering solutions for diagnostics, personalized medicine, drug discovery, patient monitoring, and more. AI professionals can help hospitals optimize resource allocation, predict patient outcomes, and improve treatment efficacy.\n",
      "\n",
      "### 3. Finance and Banking\n",
      "In finance, AI applications range from fraud detection systems to algorithmic trading platforms and customer service chatbots. Professionals with an AI background can develop sophisticated risk management models or create user-friendly financial advisory tools that use machine learning algorithms for personalized advice.\n",
      "\n",
      "### 4. Retail and E-commerce\n",
      "AI is transforming the retail industry by enabling more efficient supply chain operations, enhancing customer experience through AI-powered recommendations, and optimizing inventory management based on predictive analytics.\n",
      "\n",
      "AI majors might work on projects like automated inventory management systems or developing chatbots to provide 24/7 customer support.\n",
      "\n",
      "### 5. Automotive Industry\n",
      "Autonomous vehicles (AVs) are an area where AI is rapidly advancing. Professionals in this field can develop the algorithms that enable cars to navigate safely and efficiently, contribute to real-time traffic analysis tools, or optimize vehicle diagnostics systems using machine learning models.\n",
      "\n",
      "### 6. Legal Sector\n",
      "In law firms and legal tech companies, AI professionals work on applications like document review tools for contract analysis, patent search engines, and predictive case outcome forecasting systems based on historical data from similar cases.\n",
      "\n",
      "### Challenges and Opportunities in AI Employment Prospects\n",
      "\n",
      "**Ethical concerns**: As AI technologies grow more sophisticated, ensuring they are developed responsibly becomes paramount. Professionals need to be aware of issues such as bias in algorithms, privacy protection, and transparency in decision-making processes.\n",
      "\n",
      "**Skill development**: The field is constantly evolving, necessitating continuous learning and adaptation by professionals. Keeping up with advancements in machine learning techniques, programming languages, and ethical frameworks will remain crucial for success in the AI industry.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "The employment prospects for graduates specializing in AI are very promising across diverse industries. As technology continues to integrate more deeply into our daily lives, demand for AI expertise is only likely to increase. To thrive in this field, professionals should be prepared not only with technical skills but also with an understanding of ethical considerations and a commitment to continuous learning.\n",
      "\n",
      "The future looks bright for those pursuing careers in AI, offering opportunities to make significant contributions while navigating the exciting and evolving landscape of artificial intelligence technology."
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "  \"model\": \"qwen2:7b\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Please write an article of about 1,000 words discussing the employment prospects of AI majors.\"\n",
    "    }\n",
    "  ],\n",
    "  \"stream\": True\n",
    "}\n",
    "response = requests.post(BASE_URL, json=payload, stream=True)  # Setting stream=True here tells requests not to download the response content immediately\n",
    "# Check the response status code\n",
    "if response.status_code == 200:  \n",
    "    # Iterate the response body using iter_content()\n",
    "    for chunk in response.iter_content(chunk_size=1024):  \n",
    "        if chunk:  \n",
    "            rtn = json.loads(chunk.decode('utf-8')) \n",
    "            print(rtn[\"message\"][\"content\"], end=\"\")\n",
    "else:  \n",
    "    print(f\"Error: {response.status_code}\")  \n",
    "\n",
    "# close the response\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbcdd16",
   "metadata": {},
   "source": [
    "### 3.3 Local Chat config : Using LlamaIndex's `Ollama` Wrapper\n",
    "\n",
    "In the previous section, we used the `requests` library to interact with the local model via its RESTful API.  \n",
    "That confirmed that the **Ollama server is up and running** and capable of generating text responses.\n",
    "\n",
    "Now that we know the local setup works, let's switch to a **higher-level interface**:  \n",
    "LlamaIndex's `Ollama` wrapper, which provides a cleaner and more structured way to interact with local models using the same LLM abstraction used for OpenAI or other providers.\n",
    "\n",
    "This wrapper:\n",
    "- Removes the need for manual JSON construction and parsing\n",
    "- Supports both full and streaming generation\n",
    "- Integrates seamlessly with the rest of the LlamaIndex ecosystem (retrievers, query engines, etc.)\n",
    "\n",
    "Let‚Äôs see how we can use it to generate responses using the same local model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf66c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(base_url=\"http://127.0.0.1:11434\", model=\"qwen2:7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c41b8e",
   "metadata": {},
   "source": [
    "### 3.4 Local Chat test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0004ee27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Qwen, an AI developed by Alibaba Cloud. I'm designed to assist users in generating various types of text, such as articles, poems, and code snippets. My primary function is to provide assistance and support for a wide range of tasks and inquiries. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"Who are youÔºü\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b92e133",
   "metadata": {},
   "source": [
    "### 3.5 Local Embedding model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc7731f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "ollama_embedding = OllamaEmbedding(base_url=\"http://127.0.0.1:11434\", model_name=\"qwen2:7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c403475e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3584, list)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = ollama_embedding.get_text_embedding(\"‰Ω†Â•ΩÂëÄÂëÄ\")\n",
    "len(emb), type(emb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab5917f",
   "metadata": {},
   "source": [
    "###  3.6 Embedding Quality Test: Comparing Similarity Scores\n",
    "\n",
    "According to @DataWhale that embeddings from Ollama produce **unusual similarity scores**, which may hurt retrieval accuracy in RAG.\n",
    "\n",
    "To test this, we compare **cosine similarity** between:\n",
    "\n",
    "- A pair of **similar sentences**\n",
    "- A pair of **unrelated sentences**\n",
    "\n",
    "Using embeddings generated from different providers:\n",
    "- Ollama (local)\n",
    "- OpenAI (baseline)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f34bd7",
   "metadata": {},
   "source": [
    "#### 3.6.1 Test Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9cc038e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Hi! What a great day\"\n",
    "text2 = \"Helloooooo\"               # semantically similar\n",
    "text3 = \"I'm gay\"           # semantically different"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4febd5",
   "metadata": {},
   "source": [
    "#### 3.6.2 Cosine Similarity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6491be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    return cosine_similarity([a], [b])[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1818041",
   "metadata": {},
   "source": [
    "#### 3.6.3 Get OpenAI's Embedding's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfcf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_emb1 = embedding.get_text_embedding(text1)\n",
    "openai_emb2 = embedding.get_text_embedding(text2)\n",
    "openai_emb3 = embedding.get_text_embedding(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7109bd1",
   "metadata": {},
   "source": [
    "#### 3.6.4 Get Ollama's Embedding's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58b3b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_emb1 = ollama_embedding.get_text_embedding(text1)\n",
    "ollama_emb2 = ollama_embedding.get_text_embedding(text2)\n",
    "ollama_emb3 = ollama_embedding.get_text_embedding(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f9686",
   "metadata": {},
   "source": [
    "#### 3.6.5 Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53e0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Similarity (text1 vs text2): 0.4999185730251407\n",
      "OpenAI Similarity (text1 vs text3): 0.20050592158419314\n",
      "Ollama Similarity (text1 vs text2): 0.6257088231473826\n",
      "Ollama Similarity (text1 vs text3): 0.591558301991839\n"
     ]
    }
   ],
   "source": [
    "print(\"OpenAI Similarity (text1 vs text2):\", cos_sim(openai_emb1, openai_emb2))\n",
    "print(\"OpenAI Similarity (text1 vs text3):\", cos_sim(openai_emb1, openai_emb3))\n",
    "\n",
    "print(\"Ollama Similarity (text1 vs text2):\", cos_sim(ollama_emb1, ollama_emb2))\n",
    "print(\"Ollama Similarity (text1 vs text3):\", cos_sim(ollama_emb1, ollama_emb3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
